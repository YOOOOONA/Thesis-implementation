{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression은 x->y로 한방에 차원축소를 시키고 바이어스를 더한결과에 소프트맥스를 취한거지만\n",
    "#MLP(멀티레이어 퍼셉트론) 는 한방에 줄이지 않고 히든레이어를 여러개 두며 각 레이어마다 activation function연산을 거치게 해서\n",
    "#linear하지않게 복잡한 함수로 결과를 도출해내게 도와줌의 차이가 있다. 그리고 우리 예제에서는 MLP연산을 거친 logit에 softmax를 거쳐서 값을 도출하지않고, logit자체를 텐서함수를 거치는 인풋으로 만들겠다\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.example.tutorials.mnist import input_data\n",
    "print(\"PACKAGES LOADED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD MNIST\n",
    "mnist=input_data.read_data_sets('data/',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE MODEL\n",
    "#NETWORK TOPOLOGIES\n",
    "n_input=784\n",
    "n_hidden_1=256\n",
    "n_hidden_2=128\n",
    "n_classes=10\n",
    "\n",
    "#INPUTS AND OUTPUTS\n",
    "x=tf.placeholder(\"float\",[None, n_input])\n",
    "y=tf.placeholder(\"float\",[None, n_classes])\n",
    "\n",
    "#NETWORK PARAMETERS\n",
    "stddev=0.1#이게뭐지?\n",
    "weights={\n",
    "    'h1':tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=stddev)),\n",
    "    'h2':tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2], stddev=stddev)),\n",
    "    'out':tf.Variable(tf.random_normal([n_hidden_2,n_classes],stddev=stddev))\n",
    "}\n",
    "biases={\n",
    "    'b1':tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2':tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out':tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "print(\"NETWORK READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE GRAPH\n",
    "#MODEL\n",
    "def multilayer_perceptron(_x, _weights, _biases):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(_x,_weights['h1']),_biases['b1']))#밑줄을왜..그냥!구분할라고!\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1,_weights['h2']),_biases['b2']))\n",
    "    return (tf.matmul(layer_2,_weights['out'])+_biases['out'])#logit값으로 출력. 마지막에 소프트맥스안해줌.ak만약에 nn.softmax한번더거치면 오히려 정확도 떨어진대\n",
    "\n",
    "#PREDICTION\n",
    "pred=multilayer_perceptron(x,weights,biases)\n",
    "\n",
    "#LOSS AND OPTIMISER\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=pred))\n",
    "optm = tf.train.AdamOptimizer(learning_rate=0.001).minimize(coast)\n",
    "corr = tf.equal(tf.argmax(pred,1),tf.argmax(y,1))#pred에서 젤큰거랑 y에서 젤큰거랑 같은지 비교\n",
    "accr = tf.reduce_mean(tf.cast(corr,\"float\"))#tf.cast는 텐서를새로운 형으로 캐스팅하는 함수임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#argmax예제..\n",
    "'''\n",
    "a:\n",
    "\n",
    " [[ 3 10  1]\n",
    "\n",
    " [ 4  5  6]\n",
    "\n",
    " [ 0  8  7]]\n",
    "\n",
    "인덱스의 개수 =  2\n",
    "\n",
    "tf.argmax(a, 0): 인덱스  [1 0 2] 가 가장 큽니다.\n",
    "\n",
    "tf.argmax(a, 1): 인덱스  [1 2 1] 가 가장 큽니다\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN\n",
    "#PARAMETERS\n",
    "training_epochs=20\n",
    "batch_size=100\n",
    "display_step=4\n",
    "\n",
    "#LAUNCH THE GRAPH\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "#OPTIMIZE\n",
    "for epoch in range(training_ephochs):\n",
    "    avg_cost=0.\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    #ITERATION\n",
    "    for i in range(total_batch):\n",
    "        batch_xs. batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feeds={x:batch_xs, y:batch_ys}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "        avg_cost+=sess.run(cost, feed_dict=feeds)\n",
    "    avg_cost = avg_cost/total_batch\n",
    "    \n",
    "    #DISPLAY\n",
    "    if(epoch+1)%display_step==0:\n",
    "        print(\"EPOCH : %03d/%03d cost: %.9f\"%(epoch,training_epochs,avg_cost))\n",
    "        feeds={x:batch_xs, y:batch_ys}\n",
    "        train_acc=sess.rn(accr, feed_dict=feeds)\n",
    "        print(\"TRAIN ACCURACY: %.3f\" %(train_acc))\n",
    "        feeds={x:mnist.test.images, y:mnist.test.labels}\n",
    "        test_acc=sess.run(accr,feed_dict=feeds)\n",
    "        print(\"TEST ACCURACY: %.3f\" %(test_acc))\n",
    "print(\"OPTIMIZATION FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#로지스틱 테스티91퍼였는데  얘는 똑같이하고 방법 쫌만 바꿨는데 테스트는 97점 트레인은 100점 나옴 굳\n",
    "#w,b를 딕셔너리로 만들면 편하다. 소프트맥스안거치고 로짓으로 계산한다.\n",
    "#레이어 하나 더 추가하거나 relu로 바꾸면 성능 더 올라가는걸 확인할 수있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
